{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ca7c406",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip3 install langchain\n",
    "# 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\n",
    "# !pip3 install openai\n",
    "# !pip3 install chromadb\n",
    "# !pip3 install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37100b95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "with open(\"config.json\", \"r+\") as f : \n",
    "    config = json.load(f)\n",
    "f.close()\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = config['API_Keys']['huggingFace_token']\n",
    "os.environ[\"OPENAI_API_KEY\"] = config['API_Keys']['OpenAI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20bf2bf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader \n",
    "from langchain.embeddings import OpenAIEmbeddings \n",
    "from langchain.vectorstores import Chroma \n",
    "from langchain.chains import ConversationalRetrievalChain \n",
    "from langchain.llms import OpenAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c18bc3eb-2add-4af5-a823-78a419eedff7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System configurationConfiguration settings define the information that the Alinity s System needs to meet laboratory-specific requirements. System configuration is performed after system installation. The systemcan be reconfigured at any time if necessary.Related information...Installation procedures and special requirements, page 111Configure screen, General tab, page 113Configure screen, Computer tab, page 150Configure screen, Assay tab, page 193Configure screen, Maintenance & Diagnostics tab, page 233Utilities screen, page 244Configure screen, General tabThe operator can perform the following tasks from the General tab of the Configure screen:Users• Create new users.• Edit existing users.• Manage user PINs.• Export and import users from one Alinity s System to adifferent Alinity s System.User Profile• Change a personal user PIN.• Change a personal display theme.Reagents andSuppliesConfigure reagent and supply low alerts.Bar Codes• Enable and disable bar code types.• Configure bar code parameters.Reports• Configure automatic printing of reports.• Configure print screen outputs.System• Enable or disable the reagent and sample manager toautomatically reposition samples for retest.• Configure instrument settings.Section 2Installation procedures and special requirementsSystem configuration\n",
      "Alinity s System Operations Manual80000555-110 - 2021-10-08113\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"/Users/nandy/Downloads/Abbot_data_snipped.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "pages = loader.load_and_split()\n",
    "print(pages[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51f4dd0f-a863-4d96-9571-8a9807d7fa0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: /Users/nandy/Documents/GitHub/LLM_Research/LangChain/Abbot_data\n",
      "No embedding_function provided, using default embedding function: SentenceTransformerEmbeddingFunction\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "vectordb = Chroma.from_documents(pages, embeddings=embeddings, persist_directory=\"/Users/nandy/Documents/GitHub/LLM_Research/LangChain/Abbot_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf22ab02-d44c-48f9-a79d-ab16d2d84a0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.dump(vectordb.persist, open(\"/Users/nandy/Documents/GitHub/LLM_Research/LangChain/Abbot_data/abbottchromadb.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e8cd89a-7b87-406f-a3bf-45f26de86949",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0.9, model_name=\"gpt-3.5-turbo\"),\n",
    "                                    vectordb.as_retriever(search_kwargs={\"k\": 3}), return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c103ef8-ba46-42f9-926d-c32774194ae0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m chat_history \u001b[39m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWhat does message code 0133 mean ? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m result \u001b[39m=\u001b[39m qa({\u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m: query, \u001b[39m\"\u001b[39m\u001b[39mchat_history\u001b[39m\u001b[39m\"\u001b[39m: chat_history})\n\u001b[1;32m      5\u001b[0m result[\u001b[39m'\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qa' is not defined"
     ]
    }
   ],
   "source": [
    "#turn one \n",
    "chat_history = []\n",
    "query = \"What does message code 0133 mean ? \"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "799bbe0d-d37f-4390-92c6-354eec310880",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The sl -Msg2Fld tool may need to be run for a clean install, as mentioned by the assistant.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#turn two\n",
    "chat_history = [(query, result[\"answer\"])]\n",
    "query = \"Did they mention the tool ?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb4ab1d6-5073-4a0d-a22d-5f1c724f7239",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('What should I do for a clean install?',\n",
       "  'For a clean install, I should drop the Equip. CFG file into place and import the IP addresses. I may need to run the sl -Msg2Fld tool if necessary.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "21b23a88-e682-47d7-8364-975c71aa3751",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " What's the name?\n",
      "The tool that should be run for a clean install is the sl -Msg2Fld tool.\n",
      " Where should I go for it \n",
      "The tool that should be run for a clean install is the sl -Msg2Fld tool.\n",
      " \n",
      "The tool that should be run for a clean install is the sl -Msg2Fld tool.\n"
     ]
    }
   ],
   "source": [
    "# continous exec\n",
    "query = \" \"\n",
    "chat_history = []\n",
    "while (text != \"\"):\n",
    "    query = input()\n",
    "    result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "    chat_history.append((query, result[\"answer\"]))\n",
    "    print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "711e1684-51e4-42f0-bd8c-a3a4a7f1466c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a46e348-537d-4b52-8ff7-f6d5f490d65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0.9, model_name=\"gpt-3.5-turbo\"),\n",
    "                                    vectordb.as_retriever(search_kwargs={\"k\": 1}), return_source_documents=True)\n",
    "llm = OpenAI(OpenAI(temperature=0.9, model_name=\"gpt-3.5-turbo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb90a5cf-5236-42a5-a198-3a2651209de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_result.llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b88e8c3-ea64-4c47-9d3a-c48807099a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
